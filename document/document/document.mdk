Title         : Welcome
Author        : You
Logo          : True

[TITLE]
#Question1
## 
We have the integral
~ Equation
h(x) = \int_0^\infty e^{-x}cos(x)dx
~

In order to solve for this, we take $f(x)$ to be $e^{-x}$ and $g(x)$ to be $cos(x)$.
This is because $e^{-x}$ is a pdf. Since we can sample from $e^{-x}$, we calculate
the values of $x$ from it by using python's random number generator for $y$ in -$log(1/y)$. These random 
variables are then inserted into our $g(x)$ which is $cos(x)$.  

We know that for Monte Carlo integration:
~ Equation
\int_0^\infty g(x)f(x)dx = (1/N) \sum_{j=1}^{N} g(X_j) \text{where }  X_j \sim f
~

We get $(1/N) \sum_{j=1}^{N} g(X_j)$ to be 0.50522 

So now to check whether our breakdown of functions and the subsequent sampling 
was a good idea or not, lets calculate the Chebyshev's inequality. 

Chebyshev's inequality says
~ Equation
P \lbrack((1/N) \sum_{j=1}^{N} g(X_j) - E[g])^2 > var_{f}(g)/(N* \delta) \rbrack < \delta
~

Here $\delta = 0.01$

In order to check this, we are also given $var_{f}(g)$ to be 
~ Equation
(1/N-1) \sum_{n=1}^{N}(g(X_j) - \mu)^2 \text {where } \mu = (1/N) \sum_{n=1}^{N} g(X_j)
~

So after solving LHS in Eq3, we notice that we never have error square to be greater than
$var_{f}(g)/(N* \delta)$. This can be seen from the following plot:




#Question2 

Since the purpose of this problem is to find a decent value for $p$, lets start by takin $p=0.3$. Now we are given $N=100$ and $m=60$. 
Now, we are given that
~ Equation
Y = \sum_{k=1}^{N} Z_k
\text{where } Z_k = \begin{cases}
1  & \text{with prob } p \\
-1 & \text{with prob } 1-p \\
\end{cases}
~

For the likelihood ratio, we are given that:
~ Equation
f(Z_j)/\tilde{f}(Z_j) = \begin{cases}
1/2p  & \text{if } Z_j = 1 \\
1/2(1-p) & \text{if } Z_j = -1 \\
\end{cases}
~

After running the loop, we notice that the $ \sum Z_j $ comes out to be -34 and
the likelhood ratio turns out to be 1.834e-58. We obviously convert the ratio to
be 0 since $ \sum Z_j $ happens to be lower than $m$ which is 70. After running
the simulation 1000 times with $p = 0.3$, it becomes clear that we never reach
the desired state of $Y>70$. Here is the plot after 1000 simulations: 

![randomwalk1]

[randomwalk1]: images/randomwalk1.png "randomwalk1" { width:auto; max-width:90% }



Hence this is **not** a good choice for p.



Lets take $p = 0.7$. After running the simulation 1000 times, we notice that 
since m is very big, this biased random walk also does a terrible job. Here is 
the plot:

![randomwalk2]

[randomwalk2]: images/randomwalk2.png "randomwalk2" { width:auto; max-width:90% }



We notice that more summations of Zj are moving towards greater than 0 but there
only on 1 instance the final likelihood ratio is not zero and hence we know that on only
1 instance, we have $Y>m$. Hence, this is not a **not** a good choice for p. 


So, lets take $p=0.78$. After running the simulation 1000 times, 
following is the plot we get:

![randomwalk3]

[randomwalk3]: images/randomwalk3.png "randomwalk3" { width:auto; max-width:90% }


This biased random walking is doing an okay job of giving us some probability 
of having $Y>70$. In fact, there are a total of 50 such instances in this case.
This number can be better seen if we take log values of likelihood ratio and plot
the values again:

![randomwalk4]

[randomwalk4]: images/randomwalk4.png "randomwalk4" { width:auto; max-width:90% }

We know that $P(Y>70)$ is basically $Eg[Y]$. So we can say that $Eg[y]$ = .05.

Of course, we can keep increasing the value of p to get better probability but
then we end up making the random walk too biased. Hence it can be said said that
$p=0.78$ **is a good choice** of p without the walker being too biased.





