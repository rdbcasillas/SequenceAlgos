Title         : Assignment4
Author        : Vatsal Mehra
Logo          : True

[TITLE]
#Question1
## 
We have the integral
~ Equation
h(x) = \int_0^\infty e^{-x}cos(x)dx
~

In order to solve for this, we take $f(x)$ to be $e^{-x}$ and $g(x)$ to be $cos(x)$.
This is because $e^{-x}$ is a pdf. Since we can sample from $e^{-x}$, we calculate
the values of $x$ from it by using python's random number generator for $y$ in -$log(1/y)$
(after integrate and invert). These random variables are then inserted into our $g(x)$ which is $cos(x)$.  

Here is the plot showing why $e^{-x}$ is a decent choice for f(x):

<br>


![question1]

[question1]: images/question1.png "question1" { width:auto; max-width:90% }

<br>
  
  
We know that for Monte Carlo integration:
~ Equation
\int_0^\infty g(x)f(x)dx = (1/N) \sum_{j=1}^{N} g(X_j) \text{where }  X_j \sim f
~

The random generator used is numpy's random.exponential function. Using this gives
us the values of $x$ which are then plugged into $cos(x)$ 

We get $(1/N) \sum_{j=1}^{N} g(X_j)$ to be 0.50522 (of course this will vary a bit 
depending on random values)

So now to check whether our breakdown of functions and the subsequent sampling 
was a good idea or not, lets calculate the Chebyshev's inequality. 

Chebyshev's inequality says
~ Equation
P \lbrack((1/N) \sum_{j=1}^{N} g(X_j) - E[g])^2 > var_{f}(g)/(N* \delta) \rbrack < \delta
~

Here $\delta = 0.01$ and we also know that $E[g] = 0.5$. N was taken to be 1000. 

In order to check this, we are also given $var_{f}(g)$ to be 
~ Equation
(1/N-1) \sum_{n=1}^{N}(g(X_j) - \mu)^2 \text {where } \mu = (1/N) \sum_{n=1}^{N} g(X_j)
~


So after solving LHS in Eq3, we notice after running the experiment 
1000 times that we never have error square to be greater than
$var_{f}(g)/(N* \delta)$. This mean the probability in Equation 3 above is 0
and hence less than $\delta$. So we can conclude that we have done a good 
approximation.

Here is the plot showing satisfaction of Chebyshev's inequality:

<br>

![chebyshev1]


[chebyshev1]: images/chebyshev1.png "chebyshev1" { width:auto; max-width:90% }

<br>

Notice that all values or error are coming out to be way smaller than $var_{f}(g)/(N* \delta)$

<br>  
<br>

## 
We have the integral :
~ Equation
h(x) = \int_{-\pi}^{\pi}(1 + cos(x))^{-1/3}dx
~

In order to compute approximation to this integral, we can notice that it
cannot be sampled from like previous question since its hard to find out $g(x)$
and $f(x)$. This time, we have to take importance sampling into account.
So by multiplying and dividing lets break this $h(x)$ into $g(x)$ and 
$f(x)$ such that $g(x) = (4/cos(x/2))(1+cosx)^{-1/3}$ and $f(x) = (cos(x/2)/4$

We do this because it can be seen that integral of this f(x) over -pi to pi, 
turns out to be 1 and it also is a decent match with the plot of original h(x).

Here is h(x):

<br>

![question1b]

[question1b]: images/question1b.png "question1b" { width:auto; max-width:90% }


<br>
<br>

And here is f(x) that we chose above:

<br>

![question1b_2]

[question1b_2]: images/question1b_2.png "question1b_2" { width:auto; max-width:90% }



So we sample from this $f(x)$ after integrating and inverting the f(x) to get the
values of x. These x values are achieved from $(2*\arcsin (2*y - 1))$ where y is
sampled from python's random.uniform() generator. 

After inserting these x values in our $g(x)$, we notice that the mean of these 
values comes out to be around 6.55. 

Its important to keep in mind that here the integration after performing 
important sampling is $\sum_{j=1}^{N}g(X_j)f(X_j)/\tilde{f}{(X_j)}$

For Equation3, we have $\delta = 0.01$ and we also know that 
$E[g] = 0.564$. N was taken to be 1000. 

After checking Chebyshev inequality, we notice that sq. of error comes out 
to be approximately $0.17$ while the $var_{f}(g)/(N* \delta)$ (from equation 4) comes out to be around 1.6.
This inequality stays true even after running 1000 samples. So the probability
from Equation3 is never greater than $\delta$ which means that we have done a 
good job of approximating the integral. 

Here is the plot showing chebyshev inequality result after 1000 runs:

<br>
  
![chebyshev2]

[chebyshev2]: images/chebyshev2.png "chebyshev2" { width:auto; max-width:90% }


As can be seen from the plot, all values of error square stay lower 

<br>
<br>
<br>

#Question2 

Since the purpose of this problem is to find a decent value for $p$, lets start by takin $p=0.3$. Now we are given $N=100$ and $m=60$. 
Now, we are given that
~ Equation
Y = \sum_{k=1}^{N} Z_k
\text{where } Z_k = \begin{cases}
1  & \text{with prob } p \\
-1 & \text{with prob } 1-p \\
\end{cases}
~

For the likelihood ratio, we are given that:
~ Equation
f(Z_j)/\tilde{f}(Z_j) = \begin{cases}
1/2p  & \text{if } Z_j = 1 \\
1/2(1-p) & \text{if } Z_j = -1 \\
\end{cases}
~

After running the loop, we notice that the $ \sum Z_j $ comes out to be -34 and
the likelhood ratio turns out to be 1.834e-58. We obviously convert the ratio to
be 0 since $ \sum Z_j $ happens to be lower than $m$ which is 70. After running
the simulation 1000 times with $p = 0.3$, it becomes clear that we never reach
the desired state of $Y>70$. Here is the plot after 1000 simulations: 

<br>
  
![randomwalk1]

[randomwalk1]: images/randomwalk1.png "randomwalk1" { width:auto; max-width:90% }



Hence this is **not** a good choice for p.



Lets take $p = 0.7$. After running the simulation 1000 times, we notice that 
since m is very big, this biased random walk also does a terrible job. Here is 
the plot:

<br>

![randomwalk2]

[randomwalk2]: images/randomwalk2.png "randomwalk2" { width:auto; max-width:90% }



We notice that more summations of Zj are moving towards greater than 0 but there
only on 1 instance the final likelihood ratio is not zero and hence we know that on only
1 instance, we have $Y>m$. Hence, this is not a **not** a good choice for p. 


So, lets take $p=0.78$. After running the simulation 1000 times, 
following is the plot we get:

<br>
  
![randomwalk3]

[randomwalk3]: images/randomwalk3.png "randomwalk3" { width:auto; max-width:90% }


This biased random walking is doing an okay job of giving us some probability 
of having $Y>70$. In fact, there are a total of 50 such instances in this case.
After running the experiment with $p=0.78$, we notice that the number of 
instances hovers between 40 and 60. 

This number can be better seen if we take log values of likelihood ratio and plot
the values again:

<br>

![randomwalk4]

[randomwalk4]: images/randomwalk4.png "randomwalk4" { width:auto; max-width:90% }

We know that $P(Y>70)$ is basically $Eg[Y]$. So we can say that $Eg[y]$ = .05
when $p=0.78$.

Of course, we can keep increasing the value of p to get better probability but
then we end up making the random walk too biased. Hence it can be said said that
$p=0.78$ **is a good choice** of p without the walker being too biased.





